{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "logging.disable(sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "import jax\n",
    "\n",
    "from src.config.core import Config\n",
    "from src.config.sampler import Sampler\n",
    "from src.config.data import DatasetType\n",
    "import src.dataset as ds\n",
    "import src.training.utils as train_utils\n",
    "import src.inference.utils as inf_utils\n",
    "import src.visualization as viz\n",
    "from src.config.data import Task\n",
    "from src.inference.evaluation import evaluate_de, evaluate_bde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    DIR = Path(os.environ['SANDBOX_EXPERIMENT_DIR'])\n",
    "except KeyError as e:\n",
    "    raise KeyError('Please set the \"SANDBOX_EXPERIMENT_DIR\" variable') from e\n",
    "\n",
    "tree_dir = DIR / 'tree'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Log Summary</center></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_total_time(logs: str):\n",
    "    log_lines = [line for line in logs.split('\\n') if line]\n",
    "    # Extract Timestamp\n",
    "    if len(log_lines) < 2:\n",
    "        return 0 \n",
    "    start = re.findall(r'(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2})', log_lines[0])[0]\n",
    "    end = re.findall(r'(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2})', log_lines[-1])[0]\n",
    "    return (datetime.fromisoformat(end) - datetime.fromisoformat(start)).total_seconds()\n",
    "\n",
    "with open(DIR / 'training.log', 'r') as f:\n",
    "    logs = f.read()\n",
    "\n",
    "warm_incomplete = False\n",
    "bde_incomplete = False\n",
    "\n",
    "warmstart_match = re.findall(r'time.warmstart took (\\d+.\\d+) seconds', logs)\n",
    "bde_match = re.findall(r'time.sampling took (\\d+.\\d+) seconds', logs)\n",
    "if warmstart_match:\n",
    "    warmstart_time = float(warmstart_match[0])\n",
    "    if bde_match:\n",
    "        bde_time = float(bde_match[0])\n",
    "    else:\n",
    "        bde_time = get_total_time(logs) - warmstart_time\n",
    "        bde_incomplete = True\n",
    "else:\n",
    "    warm_incomplete = True\n",
    "    bde_incomplete = True\n",
    "    warmstart_time = get_total_time(logs)\n",
    "    bde_time = 0\n",
    "\n",
    "total_time = round(warmstart_time + bde_time, 2)\n",
    "print(\"-\" * 50)\n",
    "print(f'Warmstart time: {round(warmstart_time / 60, 2)} min{\" (incomplete)\" if warm_incomplete else \"\"}')\n",
    "print(f'BDE time: {round(bde_time / 60, 2)} min{\" (incomplete)\" if bde_incomplete else \"\"}')\n",
    "print(f'Total time: {round(total_time / 60, 2)} min{\" (incomplete)\" if warm_incomplete or bde_incomplete else \"\"}')\n",
    "print(\"-\" * 50)\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore')\n",
    "    config = Config.from_yaml(DIR / 'config.yaml')\n",
    "# print(f\"> Loaded Configuration:\\n{config}\")\n",
    "metrics = {\n",
    "    'total_time': total_time,\n",
    "    'warmstart_time': warmstart_time,\n",
    "    'bde_time': bde_time\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warmstart Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warmstart_dir = DIR / 'warmstart'\n",
    "files = [warmstart_dir / f for f in os.listdir(warmstart_dir) if f.endswith('.npz')]\n",
    "params_warmstart = train_utils.load_params_batch(files, tree_path=tree_dir)\n",
    "if len(files) == 1:\n",
    "    params_warmstart = jax.tree.map(lambda x: x[None, ...], params_warmstart)\n",
    "print(f'Number of NaNs:\\n{inf_utils.count_nan(params_warmstart)}')\n",
    "viz.plot_warmstart_results(warmstart_dir=warmstart_dir)\n",
    "# viz.plot_param_hist(params_warmstart, hist_=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Sampling Warmup</center></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.training.sampler.name == Sampler.MCLMC:\n",
    "    with open(DIR / 'warmup_params.txt', 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        step_size = jax.numpy.array([float(x) for x in lines[0].strip().split(',')])\n",
    "        L = jax.numpy.array([float(x) for x in lines[1].strip().split(',')])\n",
    "\n",
    "    print('Tuned MCLMC Parameters after warmup:')\n",
    "    print(f'Step Size: {jax.numpy.mean(step_size):.4f} ± {jax.numpy.std(step_size):.4f}')\n",
    "    print(f'L: {jax.numpy.mean(L):.4f} ± {jax.numpy.std(L):.4f}')\n",
    "    metrics['step_size'] = jax.numpy.mean(step_size)\n",
    "    metrics['L'] = jax.numpy.mean(L)\n",
    "    metrics['step_size_std'] = jax.numpy.std(step_size)\n",
    "    metrics['L_std'] = jax.numpy.std(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit memory usage\n",
    "if config.training.sampler.keep_warmup:\n",
    "    samples_warmup = train_utils.load_samples_from_dir(DIR / 'sampling_warmup', tree_path=tree_dir)\n",
    "    n_samples = inf_utils.count_samples(samples_warmup)\n",
    "    n_chains = inf_utils.count_chains(samples_warmup)\n",
    "    print(f'Loaded {n_chains} chains with {n_samples} warmup samples each')\n",
    "    print(f'Number of NaNs:\\n{inf_utils.count_nan(samples_warmup)}')\n",
    "    _ = viz.plot_effective_sample_size(samples_warmup)\n",
    "    viz.plot_param_movement(samples_warmup, random_n=5)\n",
    "    # viz.plot_param_hist(samples_warmup, hist_limit=10000)\n",
    "else:\n",
    "    print('> No warmup samples has been saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Sampling</center></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = train_utils.load_samples_from_dir(DIR / 'samples', tree_path=tree_dir)\n",
    "n_samples = inf_utils.count_samples(samples)\n",
    "n_chains = inf_utils.count_chains(samples)\n",
    "print(f'Loaded {n_chains} chains with {n_samples} samples each')\n",
    "mean_ess = viz.plot_effective_sample_size(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_bcv, mean_wcv = viz.plot_variances(samples)\n",
    "mean_crhat = viz.plot_split_chain_r_hat(samples, n_splits=4)\n",
    "# viz.plot_param_hist(samples, hist_limit=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.plot_pca(samples, '3d', max_chains=2, max_samples=None, annotate=True)\n",
    "viz.plot_param_movement(samples, random_n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the mean diagnostics into a dataframe and save it\n",
    "rows = list(mean_ess.keys())\n",
    "columns = ['ess', 'bcv', 'wcv', 'crhat']\n",
    "df = pd.DataFrame(index=rows, columns=columns)\n",
    "df['ess'] = mean_ess.values()\n",
    "df['bcv'] = mean_bcv.values()\n",
    "df['wcv'] = mean_wcv.values()\n",
    "df['crhat'] = mean_crhat.values()\n",
    "df.to_csv(DIR / 'diagnostics.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>DE vs BDE Performance</center></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra arguments needed in forward pass of the flax model\n",
    "kwargs = {}\n",
    "\n",
    "# Setup Loader\n",
    "if config.data.data_type == DatasetType.TABULAR:\n",
    "    loader = ds.TabularLoader(\n",
    "        config.data,\n",
    "        rng=config.jax_rng,\n",
    "        target_len=config.data.target_len\n",
    "    )\n",
    "elif config.data.data_type == DatasetType.TEXT:\n",
    "    from src.dataset.utils import CustomBPETokenizer\n",
    "    tokenizer = CustomBPETokenizer(\n",
    "        config.model.context_len, max_vocab_size=config.model.vocab_size\n",
    "    )\n",
    "    loader = ds.TextLoader(\n",
    "        config=config.data,\n",
    "        rng=config.jax_rng,\n",
    "        tokenizer=tokenizer,\n",
    "        context_len=config.model.context_len,\n",
    "        omit_freq=100,\n",
    "    )\n",
    "    kwargs = {'train': False, 'pad_id': tokenizer.padding_token_id}\n",
    "elif config.data.data_type == DatasetType.IMAGE:\n",
    "    loader = ds.ImageLoader(\n",
    "        config.data,\n",
    "        rng=config.jax_rng,\n",
    "    )\n",
    "else:\n",
    "    raise NotImplementedError(f'DatasetType {config.data.data_type} not supported yet.')\n",
    "\n",
    "random_input = next(loader.iter('train', 1))['feature']\n",
    "\n",
    "print(f'> Loader:\\n{loader}')\n",
    "module = config.get_flax_model()\n",
    "\n",
    "print('> Parameter Overview:\\n')\n",
    "print(module.tabulate(config.jax_rng, x=random_input, **kwargs))\n",
    "\n",
    "with open(DIR / 'samples/info.pkl', 'rb') as f:\n",
    "    info = pickle.load(f)\n",
    "\n",
    "accept_rate = info.get(\"acceptance_rate\", None)\n",
    "integration_steps = info.get(\"num_integration_steps\", None)\n",
    "if accept_rate is not None:\n",
    "    print(\"Average acceptance rate: \", accept_rate.mean())\n",
    "    metrics['acceptance_rate'] = accept_rate.mean()\n",
    "if integration_steps is not None:\n",
    "    print(\"Average ingeration steps: \", integration_steps.mean())\n",
    "    metrics['integration_steps'] = integration_steps.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = loader.test_x # (B x F)\n",
    "labels = loader.test_y # (B x T)\n",
    "print(\"Test Set: Feature and Label have shapes: \", features.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits, metrics = evaluate_de(\n",
    "    params=params_warmstart,\n",
    "    module=module,\n",
    "    features=features,\n",
    "    labels=labels,\n",
    "    task=config.data.task,\n",
    "    batch_size=config.training.warmstart.batch_size,\n",
    "    verbose=False,\n",
    "    metrics_dict=metrics,\n",
    "    n_samples=config.training.sampler.n_samples // config.training.sampler.n_thinning,\n",
    "    rng_key=config.jax_rng,\n",
    "    nominal_coverages=[0.5, 0.75, 0.9, 0.95],\n",
    "    **kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_cap = 4 * 1024 ** 3 # 4 GB\n",
    "mem_usage = (\n",
    "    inf_utils.get_mem_size(samples) / n_chains\n",
    "    + inf_utils.get_mem_size(features)\n",
    "    + inf_utils.get_mem_size(labels)\n",
    ")\n",
    "overhead_unit = mem_usage * 10 # estimatation\n",
    "\n",
    "batch_size = max(1, (mem_cap - overhead_unit) // mem_usage)\n",
    "if batch_size > labels.shape[0]:\n",
    "    batch_size = None\n",
    "print(f'> Batch size for evaluation: {batch_size}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits, metrics = evaluate_bde(\n",
    "    params=samples,\n",
    "    module=module,\n",
    "    features=features,\n",
    "    labels=labels,\n",
    "    task=config.data.task,\n",
    "    batch_size=batch_size,\n",
    "    verbose=False,\n",
    "    metrics_dict=metrics,\n",
    "    nominal_coverages=[0.5, 0.75, 0.9, 0.95],\n",
    "    **kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LPPD over the samples\n",
    "viz.plot_lppd(logits, labels, config.data.task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save metrics\n",
    "with open(DIR / 'metrics.pkl', 'wb') as f:\n",
    "    pickle.dump(metrics, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
